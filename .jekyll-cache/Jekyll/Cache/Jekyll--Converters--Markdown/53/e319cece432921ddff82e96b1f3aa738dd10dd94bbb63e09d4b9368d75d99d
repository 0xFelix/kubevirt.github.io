I"v'<p>In this <a href="https://www.youtube.com/watch?v=Qejlyny0G58">video</a>, David and Vishesh explore the architecture behind KubeVirt and how NVIDIA is leveraging that architecture to power GPU workloads on Kubernetes.
Using NVIDIA’s GPU workloads as a case of study, they provide a focused view on how host device passthrough is accomplished with KubeVirt as well as providing some
performance metrics comparing KubeVirt to standalone KVM.</p>

<h2 id="kubevirt-intro">KubeVirt Intro</h2>

<p>David introduces the talk showing what KubeVirt is and what is not:</p>

<ul>
  <li>KubeVirt is not involved with managing AWS or GCP instances</li>
  <li>KubeVirt is not a competitor to Firecracker or Kata containers</li>
  <li>KubeVirt is not a container runtime replacement</li>
</ul>

<p>He likes to define KubeVirt as:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>KubeVirt is a Kubernetes extension that allows running traditional VM workloads natively side by side with Container workloads.</p>


</div></div>
<p>But why KubeVirt?</p>

<ul>
  <li>Already have on-premise solutions like OpenStack, oVirt</li>
  <li>And then there’s the public cloud, AWS, GCP, Azure</li>
  <li>Why are we doing this VM management stuff yet again?</li>
</ul>

<p>The answer is that the initial motivation for it was this idea of infrastructure convergence:</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_infrastructure_convergence.png" alt="kubevirt_infrastructure_convergence" title="KubeVirt infrastructure convergence" /></p>

<p>The transition to the cloud model involves multiple stacks, containers and VMs, old code and new code.
With KubeVirt all this is simplified with just one stack to manage containers and VMs to run old code and new code.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_one_stack.png" alt="kubevirt_one_stack" title="KubeVirt one stack" /></p>

<p>The workflow convergence means that:</p>

<ul>
  <li>Converging VM management into container management workflows</li>
  <li>Using the same tooling (kubectl) for containers and Virtual Machines</li>
  <li>Keeping the declarative API for VM management (just like pods, deployments, etc…)</li>
</ul>

<p>An example of a VM Instance in YAML could be so simple as the following example:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ cat &lt;&lt;EOF | kubectl create -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="nn">...</span>
 <span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
   <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">cores</span><span class="pi">:</span> <span class="m">2</span>
   <span class="na">devices</span><span class="pi">:</span>
    <span class="na">disk</span><span class="pi">:</span> <span class="s">fedora29</span>
</code></pre></div></div>

<h2 id="architecture">Architecture</h2>

<p>The truth here is that a KubeVirt VM is a KVM+qemu process running inside a pod. It’s as simple as that.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_virtual_machine.png" alt="kubevirt_virtual_machine" title="KubeVirt VM = KVM+qemu" /></p>

<p>The VM Launch flow is shown in the following diagram. Since the user posts a VM manifest to the cluster until the Kubelet spins up the VM pod.
And finally the virt-handler instructs the virt-launcher how to launch the qemu.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_vm_launch_flow.png" alt="kubevirt_vm_launch_flow" title="KubeVirt VM launch flow" /></p>

<p>The storage in KubeVirt is used in the same way as the pods, if there is a need to have persistent storage in a VM a PVC (Persistent Volume Claim)
needs to be created.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_volumes.png" alt="kubevirt_volumes" title="KubeVirt volumes" /></p>

<p>For example, if you have a VM in your laptop, you can upload that image using the <a href="https://github.com/kubevirt/containerized-data-importer">containerized-data-importer</a> (CDI) to a PVC and then you can attach
that PVC to the VM pod to get it running.</p>

<p>About the use of network services, the traffic routes to the KubeVirt VM in the same way it does to container workloads. Also with Multus there is
the possibility to have different network interfaces per VM.</p>

<p>For using the Host Resources:</p>

<ul>
  <li>VM Guest CPU and NUMA Affinity
    <ul>
      <li>CPU Manager (pinning)</li>
      <li>Topology Manager (NUMA nodes)</li>
    </ul>
  </li>
  <li>VM Guest CPU/MEM requirements
    <ul>
      <li>POD resource request/limits</li>
    </ul>
  </li>
  <li>VM Guest use of Host Devices
    <ul>
      <li>Device Plugins for access to (/dev/kvm, SR-IOV, GPU passthrough)</li>
      <li>POD resource request/limits for device allocation</li>
    </ul>
  </li>
</ul>

<h2 id="gpuvgpu-in-kubevirt-vms">GPU/vGPU in Kubevirt VMs</h2>

<p>After the introduction of David, Vishesh takes over and talks in-depth the whys and hows of GPUs in VM. Lots of new Machine and Deep learning applications
are taking advance of the GPU workloads. Nowadays the Big data is one of the main consumers of GPUs but there are some gaps, the gaming and professional graphics sector
still need to run VMs and have native GPU functionalities, that is why NVIDIA decided to work with KubeVirt.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpus_on_kubevirt.png" alt="gpus_on_kubevirt" title="GPU/vGPU on KubeVirt" /></p>

<p>To enable the device pass-through NVIDIA has developed the KubeVirt GPU device Plugin, it is available in <a href="https://github.com/NVIDIA/kubevirt-gpu-device-plugin">GitHub</a>
It’s open-source and anybody can take a look to it and download it.</p>

<p>Using the device plugin framework is a natural choice to provide GPU access to Kubevirt VMs,
the following diagram shows the different layers involved in the GPU pass-through architecture:</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/kubevirt_gpu_passthrough.png" alt="kubevirt_gpu_passthrough" title="KubeVirt GPU passthrough" /></p>

<p>Vishesh also comments an example of a YAML code where it can be seen the Node Status containing the NVIDIA card information (5 GPUs in that node), the Virtual Machine specification
containing the <code class="highlighter-rouge">deviceName</code> that points to that NVIDIA card and also the Pod Status where the user can set the limits and request for that resource as
any other else in Kubernetes.</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpu_pass_yaml.png" alt="kubevirt_gpu_pass_yaml" title="KubeVirt GPU passthrough yaml" /></p>

<p>The main Device Plugin Functions are:</p>

<ul>
  <li>GPU and vGPU device Discovery
    <ul>
      <li>GPUs with VFIO-PCI driver on the host are identified</li>
      <li>vGPUs configured using NVIDIA vGPU manager are identified</li>
    </ul>
  </li>
  <li>GPU and vGPU device Advertising
    <ul>
      <li>Discovered devices are advertised to kubelet as allocatable resources</li>
    </ul>
  </li>
  <li>GPU and vGPU device Allocation
    <ul>
      <li>Returns the PCI address of allocated GPU device</li>
    </ul>
  </li>
  <li>GPU and vGPU Health Check
    <ul>
      <li>Performs health check on the discovered GPU and vGPU devices</li>
    </ul>
  </li>
</ul>

<p>To understand how the GPU passthrough lifecycle works Vishesh shows the different phases involve in the process using the following diagram:</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/gpu_pass_lifecycle.png" alt="gpu_pass_lifecycle" title="KubeVirt GPU passthrough lifecycle" /></p>

<p>In the following diagram there are some of the Key features that NVIDIA is using with KubeVirt:</p>

<p><img src="/assets/2020-02-06-KubeVirt_deep_dive-virtualized_gpu_workloads/NVIDIA_usecase_keyfeatures.png" alt="NVIDIA_usecase_keyfeatures" title="KubeVirt NVIDIA usecase keyfeatures" /></p>

<p>If you are interested in the details of how the lifecycle works or in why NVIDIA is highly using some of the KubeVirt features listed above, you may be interested in
taking a look to the following video.</p>

<h2 id="video">Video</h2>

<iframe width="560" height="315" style="height: 315px" src="https://www.youtube.com/embed/Qejlyny0G58" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="speakers">Speakers</h2>

<p><a href="https://github.com/davidvossel">David Vossel</a> David Vossel is a Principal Software Engineer at Red Hat. He is currently working on OpenShift’s Container Native Virtualization (CNV)
and is a core contributor to the open source KubeVirt project.</p>

<p><a href="https://www.linkedin.com/in/vishesh-tanksale">Vishesh Tanksale</a> is currently a Senior Software Engineer at NVIDIA. He is focussing on different aspects of enabling VM workload management on Kubernetes Cluster.
He is specifically interested in GPU workloads on VMs. He is an active contributor to Kubevirt, a CNCF Sandbox Project.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://www.youtube.com/watch?v=Qejlyny0G58">YouTube video: KubeVirt Deep Dive: Virtualized GPU Workloads on KubeVirt - David Vossel, Red Hat &amp; Vishesh Tanksale, NVIDIA</a></li>
  <li><a href="https://static.sched.com/hosted_files/kccncna19/31/KubeCon%202019%20-%20Virtualized%20GPU%20Workloads%20on%20KubeVirt.pdf">Presentation: Virtualized GPU workloads on KubeVirt</a></li>
  <li><a href="https://kccncna19.sched.com/event/VnjX">KubeCon NA 2019 event</a></li>
</ul>
:ET