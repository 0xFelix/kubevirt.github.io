I",<h1 id="kubevirt-memory-overcommitment">KubeVirt memory overcommitment</h1>

<p>One of the latest additions to KubeVirt has been the memory overcommitment feature which allows the memory being assigned to a Virtual Machine Instance to be different than what it requests to Kubernetes.</p>

<h2 id="what-it-does">What it does</h2>

<p>As you might already know, when a pod is created in Kubernetes, it can define requests for resources like CPU or memory, those requests are taken into account for deciding to what node the pod will be scheduled. Usually, on a node, there are already some resources reserved or requested, Kubernetes itself <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">reserves some resources for its processes</a> and there might be monitoring pods or storage pods already requesting resources as well, all those are also accounted for what is left to run pods.</p>

<p>Having the memory overcommitment feature included in KubeVirt allows the users to assign the VMI more or less memory than set into the requests, offering more flexibility, giving the user the option to overcommit (or undercommit) the node’s memory if needed.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>It’s not too complex to get this working, all that is needed is to have, at least, KubeVirt version <em>0.8.0</em> installed, which includes the aforementioned feature, and use the following settings on the VMI definition:</p>

<ul>
  <li><code class="highlighter-rouge">domain.memory.guest</code>: Defines the amount memory assigned to the VMI process (by libvirt).</li>
  <li><code class="highlighter-rouge">domain.resources.requests.memory</code>: Defines the memory requested to Kubernetes by the pod that will run the VMI.</li>
  <li><code class="highlighter-rouge">domain.resources.overcommitGuestOverhead</code>: Boolean switch to enable the feature.</li>
</ul>

<p>Once those are in place, Kubernetes will consider the requested memory for scheduling while libvirt will define the domain with the amount of memory defined in <code class="highlighter-rouge">domain.memory.guest</code>. For example, let’s define a VMI which requests <em>24534983Ki</em> but wants to use <em>25761732Ki</em>
instead.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha2</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">testvm1</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">memory</span><span class="pi">:</span>
      <span class="na">guest</span><span class="pi">:</span> <span class="s2">"</span><span class="s">25761732Ki"</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">24534983Ki"</span>
      <span class="na">overcommitGuestOverhead</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">volumeName</span><span class="pi">:</span> <span class="s">myvolume</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">mydisk</span>
          <span class="na">disk</span><span class="pi">:</span>
            <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">volumeName</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
          <span class="na">cdrom</span><span class="pi">:</span>
            <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">myvolume</span>
      <span class="na">registryDisk</span><span class="pi">:</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">&lt;registry_address&gt;/kubevirt/fedora-cloud-registry-disk-demo:latest</span>
    <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
      <span class="na">userData</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">#cloud-config</span>
        <span class="s">hostname: testvm1</span>
        <span class="s">users:</span>
          <span class="s">- name: kubevirt</span>
            <span class="s">gecos: KubeVirt Project</span>
            <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
            <span class="s">passwd: $6$JXbc3063IJir.e5h$ypMlYScNMlUtvQ8Il1ldZi/mat7wXTiRioGx6TQmJjTVMandKqr.jJfe99.QckyfH/JJ.OdvLb5/OrCa8ftLr.</span>
            <span class="s">shell: /bin/bash</span>
            <span class="s">home: /home/kubevirt</span>
            <span class="s">lock_passwd: false</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
</code></pre></div></div>

<p>As explained already, the QEMU process spawn by libvirt, will get <em>25761732Ki</em> of RAM, minus some amount for the graphics and firmwares, the guest OS will see its total memory close to that amount, while Kubernetes would think the pod requests <em>24534983Ki</em>, making more room to schedule more pods if needed.</p>

<p>Now let’s imagine we want to undercommit, here’s the same YAML definition but setting less memory than requested:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha2</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">testvm1</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">memory</span><span class="pi">:</span>
      <span class="na">guest</span><span class="pi">:</span> <span class="s2">"</span><span class="s">23308234Ki"</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">24534983Ki"</span>
      <span class="na">overcommitGuestOverhead</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">volumeName</span><span class="pi">:</span> <span class="s">myvolume</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">mydisk</span>
          <span class="na">disk</span><span class="pi">:</span>
            <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">volumeName</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
          <span class="na">cdrom</span><span class="pi">:</span>
            <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">myvolume</span>
      <span class="na">registryDisk</span><span class="pi">:</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">&lt;registry_url&gt;/kubevirt/fedora-cloud-registry-disk-demo:latest</span>
    <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
        <span class="na">userData</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">#cloud-config</span>
          <span class="s">hostname: testvm1</span>
          <span class="s">users:</span>
            <span class="s">- name: kubevirt</span>
              <span class="s">gecos: KubeVirt Project</span>
              <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
              <span class="s">passwd: $6$JXbc3063IJir.e5h$ypMlYScNMlUtvQ8Il1ldZi/mat7wXTiRioGx6TQmJjTVMandKqr.jJfe99.QckyfH/JJ.OdvLb5/OrCa8ftLr.</span>
              <span class="s">shell: /bin/bash</span>
              <span class="s">home: /home/kubevirt</span>
              <span class="s">lock_passwd: false</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitvolume</span>
</code></pre></div></div>

<h2 id="why-this-is-needed">Why this is needed</h2>

<p>At this point you might be asking yourself why would this feature be needed if Kubernetes already does resource management for you, right? Well, there might be few scenarios where this feature would be needed, for instance imagine you decide to have a cluster or few nodes completely dedicated to run Virtual Machines, this feature allows you to make use of all the memory in the nodes without really accounting for the already reserved or requested memory in the system.</p>

<p>Let’s put it as an example, say a node has 100GiB of RAM, with 2GiB of reserved memory plus 1GiB requested by monitoring and storage pods, that leaves the user 97GiB of allocatable memory to schedule pods, so each VMI that needs to be started on a node needs to request an amount that would fit, if the user wants to run 10 VMIs on each node with 10GiB of RAM Kubernetes wouldn’t allow that cause the sum of their requests would be more than what’s allocatable in the node.</p>

<p>Using the memory overcommitment feature the user can tell Kubernetes that each VMI requests 9.7GiB and set <code class="highlighter-rouge">domain.memory.guest</code> to 10GiB.</p>

<p>The other way around, undercommitting the node, also works, for instance, to make sure that no matter how many VMIs will be under memory pressure the node will still be in good shape. Using the same node sizing, 100GiB, we could define 10 VMIs to request 9.7GiB, while giving them exactly 9.0GiB, that’d leave around 7GiB for the node processes while Kubernetes wouldn’t try to schedule any more pods on it cause all the requests already sum up to 100% of the allocatable memory.</p>
:ET