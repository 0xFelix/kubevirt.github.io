I"Å)<p>This blog post shows how KubeVirt can take advantage of Kubernetes inner features to provide an advanced scheduling mechanism to virtual machines (VMs). The same or even more complex <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">affinity and anti-affinity</a> rules can be assigned to VMs or Pods in Kubernetes than in traditional virtualization solutions.</p>

<p>It is important to notice that from the Kubernetes scheduler stand point, which will be explained later, it only manages Pod and node scheduling. Since the VM is wrapped up in a Pod, the same scheduling rules are completely valid to KubeVirt VMs.</p>

<div class="premonition warning"><div class="fa fa-exclamation-circle"></div><div class="content"><p class="header">Warning</p><p>As informed in the <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity">official Kubernetes documentation</a>: inter-pod affinity and anti-affinity require substantial amount of processing which can slow-down scheduling in large clusters significantly. This can be specially notorious in clusters larger than several hundred nodes.</p>


</div></div>
<h2 id="introduction">Introduction</h2>

<p>In a Kubernetes cluster, <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a> is the default scheduler and runs as part of the control plane. Kube-scheduler is in charge of selecting an optimal node for every newly created or unscheduled pod to run on. However, every container within a pod and the pods themselves, have different requirements for resources. Therefore, existing nodes need to be filtered according to the specific requirements.</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>If you want and need to, you can write your own scheduling component and use it instead.</p>


</div></div>
<p>When we talk about scheduling, we refer basically to making sure that Pods are matched to Nodes so that a Kubelet can run them. Actually, kube-scheduler selects a node for the pod in a 2-step operation:</p>

<ul>
  <li><strong>Filtering.</strong> The filtering step finds the set of candidate Nodes where it‚Äôs possible to schedule the Pod. The result is a list of Nodes, usually more than one.</li>
  <li><strong>Scoring.</strong> In the scoring step, the scheduler ranks the remaining nodes to choose the most suitable Pod placement. This is accomplished based on a score obtained from a list of scoring rules that are applied by the scheduler.</li>
</ul>

<p>The obtained list of candidate nodes is evaluated using multiple priority criteria, which add up to a weighted score. Nodes with higher values are better candidates to run the pod. Among the criteria are affinity and anti-affinity rules; nodes with higher affinity for the pod have a higher score, and nodes with higher anti-affinity have a lower score. Finally, kube-scheduler assigns the Pod to the Node with the highest score. If there is more than one node with equal scores, kube-scheduler selects one of these randomly.</p>

<p>In this blog post we are going to focus on examples of affinity and anti-affinity rules applied to solve real use cases. A common use for affinity rules is to schedule related pods to be close to each other for performance reasons. A common use case for anti-affinity rules is to schedule related pods not too close to each other for high availability reasons.</p>

<h2 id="goal-run-my-customapp">Goal: Run my customapp</h2>

<p>In this example, our mission is to run a customapp that is composed of 3 tiers:</p>

<ol>
  <li>A web proxy cache based on varnish HTTP cache.</li>
  <li>A web appliance delivered by a third provider.</li>
  <li>A clustered database running on MS Windows.</li>
</ol>

<p>Instructions were delivered to deploy the application in our production Kubernetes cluster taking advantage of the existing KubeVirt integration and making sure the application is resilient to any problems that can occur. The current status of the cluster is the following:</p>

<ul>
  <li>A stretched Kubernetes cluster is already up and running.</li>
  <li><a href="https://kubevirt.io/user-guide/#/">KubeVirt</a> is already installed.</li>
  <li>There is enough free CPU, Memory and disk space in the cluster to deploy customapp stack.</li>
</ul>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">information</p><p>The Kubernetes stretched cluster is running in 3 different geographical locations to provide high availability. Also, all locations are close and well-connected to provide low latency between the nodes.</p>


</div></div>
<p>Topology used is common for large data centers, such as cloud providers, which is based in organizing hosts into regions and zones:</p>

<ul>
  <li>A <strong>region</strong> is a set of hosts in a close geographic area, which guarantees high-speed connectivity between them.</li>
  <li>A <strong>zone</strong>, also called an availability zone, is a set of hosts that might fail together because they share common critical infrastructure components, such as a network, storage, or power.</li>
</ul>

<p>There are some important labels when creating advanced scheduling workflows with affinity and anti-affinity rules. As explained previously, they are very close linked to common topologies used in datacenters. Labels such as:</p>

<ul>
  <li><em>topology.kubernetes.io/zone</em></li>
  <li><em>topology.kubernetes.io/region</em></li>
  <li><em>kubernetes.io/hostname</em></li>
  <li><em>kubernetes.io/arch</em></li>
  <li><em>kubernetes.io/os</em></li>
</ul>

<div class="premonition warning"><div class="fa fa-exclamation-circle"></div><div class="content"><p class="header">Warning</p><p>As it is detailed in the <a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/">labels and annotations official documentation</a>, starting in v1.17, label <em>failure-domain.beta.kubernetes.io/region</em> and <em>failure-domain.beta.kubernetes.io/zone</em> are deprecated in favour of <strong>topology.kubernetes.io/region</strong> and <strong>topology kubernetes.io/zone respectively</strong>.</p>


</div></div>
<p>Previous labels are just prepopulated Kubernetes labels that the system uses to denote such a topology domain. In our case, the cluster is running in <em>Iberia</em> <strong>region</strong> across three different <strong>zones</strong>: <em>scu, bcn and sab</em>. Therefore, it must be labelled accordingly since advanced scheduling rules are going to be applied:</p>

<p><img src="/assets/2020-02-25-Advanced-scheduling-with-affinity-rules/kubevirt-blog-affinity.resized.png" alt="cluster labelling" /></p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Pod anti-affinity requires nodes to be consistently labelled, i.e. every node in the cluster must have an appropriate label matching <strong>topologyKey</strong>. If some or all nodes are missing the specified topologyKey label, it can lead to unintended behavior.</p>


</div></div>
<p>Below you can find a cluster labeling where topology is based in one region and several zones spread across geographically. Additionally, special <strong>high performing nodes</strong> composed by nodes with a high number of resources available including memory, cpu, storage and network are marked as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl label node kni-worker topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>scu
node/kni-worker labeled
<span class="nv">$ </span>kubectl label node kni-worker2 topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>scu <span class="nv">performance</span><span class="o">=</span>high
node/kni-worker2 labeled
<span class="nv">$ </span>kubectl label node kni-worker3 topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>bcn
node/kni-worker3 labeled
<span class="nv">$ </span>kubectl label node kni-worker4 topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>bcn <span class="nv">performance</span><span class="o">=</span>high
node/kni-worker4 labeled
<span class="nv">$ </span>kubectl label node kni-worker5 topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>sab
node/kni-worker5 labeled
<span class="nv">$ </span>kubectl label node kni-worker6 topology.kubernetes.io/region<span class="o">=</span>iberia topology.kubernetes.io/zone<span class="o">=</span>sab <span class="nv">performance</span><span class="o">=</span>high
node/kni-worker6 labeled
</code></pre></div></div>

<p>At this point, Kubernetes cluster nodes are labelled as expected:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get nodes <span class="nt">--show-labels</span>

NAME                STATUS   ROLES    AGE   VERSION   LABELS
kni-control-plane   Ready    master   18m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-control-plane,kubernetes.io/os<span class="o">=</span>linux,node-role.kubernetes.io/master<span class="o">=</span>
kni-worker          Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker,kubernetes.io/os<span class="o">=</span>linux,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>scu
kni-worker2         Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker2,kubernetes.io/os<span class="o">=</span>linux,performance<span class="o">=</span>high,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>scu
kni-worker3         Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker3,kubernetes.io/os<span class="o">=</span>linux,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>bcn
kni-worker4         Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker4,kubernetes.io/os<span class="o">=</span>linux,performance<span class="o">=</span>high,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>bcn
kni-worker5         Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker5,kubernetes.io/os<span class="o">=</span>linux,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>sab
kni-worker6         Ready    &lt;none&gt;   17m   v1.17.0   beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kni-worker6,kubernetes.io/os<span class="o">=</span>linux,performance<span class="o">=</span>high,topology.kubernetes.io/region<span class="o">=</span>iberia,topology.kubernetes.io/zone<span class="o">=</span>sab
</code></pre></div></div>

<p>Finally, the cluster is ready to run and deploy our specific <em>customapp</em>.</p>

<h3 id="the-clustered-database">The clustered database</h3>

<p>A MS Windows 2016 Server virtual machine is already containerized and ready to be deployed. As we have to deploy 3 replicas of the operating system a <code class="highlighter-rouge">VirtualMachineInstanceReplicaSet</code> has been created. Once the replicas are up and running, database administrators will be able to reach the VMs running in our Kubernetes cluster through Remote Desktop Protocol (RDP). Eventually, MS SQL2016 database is installed and configured as a clustered database to provide high availability to our customapp.</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Check <a href="/2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html">KubeVirt: installing Microsoft Windows from an ISO</a> if you need further information on how to deploy a MS Windows VM on KubeVirt.</p>


</div></div>
<p>Regarding the scheduling, a Kubernetes node of each zone has been labelled as high-performance, e.g. it has more memory, storage, CPU and higher performing disk and network than the other node that shares the same zone. This specific Kubernetes node was provisioned to run the database VM due to the hardware requirements to run database applications. Therefore, a scheduling rule is needed to be sure that all MSSQL2016 instances run <em>only</em> in these high-performance servers.</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>These nodes were labelled as <strong>performance=high</strong>.</p>


</div></div>
<p>There are two options to accomplish our requirement, use <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">nodeSelector</a> or configure <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity">nodeAffinity</a> rules. In our first approach, <code class="highlighter-rouge">nodeSelector</code> instead of <code class="highlighter-rouge">nodeAffinity</code> rule is used. <code class="highlighter-rouge">nodeSelector</code> matches the nodes where the <code class="highlighter-rouge">performance</code> key is equal to <code class="highlighter-rouge">high</code> and makes the <code class="highlighter-rouge">VirtualMachineInstance</code> to run on top of the matching nodes. The following code snippet shows the configuration:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstanceReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mssql2016</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">mssql2016</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mssql2016</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">mssql2016</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span> <span class="c1">#nodeSelector matches nodes where performance key has high as value.</span>
        <span class="na">performance</span><span class="pi">:</span> <span class="s">high</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">16Gi</span>
      <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>Next, the <code class="highlighter-rouge">VirtualMachineInstanceReplicaSet</code> configuration partially shown previously is applied successfully.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl create <span class="nt">-f</span> vmr-windows-mssql.yaml
virtualmachineinstancereplicaset.kubevirt.io/mssql2016 created
</code></pre></div></div>

<p>Then, it is expected that the 3 <code class="highlighter-rouge">VirtualMachineInstances</code> will eventually run on the nodes where matching key/value label is configured. Actually, based on the hostname those are the <em>even</em> nodes.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME                                 READY   STATUS              RESTARTS   AGE   IP       NODE          NOMINATED NODE   READINESS GATES
virt-launcher-mssql2016p948r-257pn   0/2     ContainerCreating   0          16s   &lt;none&gt;   kni-worker4   &lt;none&gt;           &lt;none&gt;
virt-launcher-mssql2016rd4lk-6zz9d   0/2     ContainerCreating   0          16s   &lt;none&gt;   kni-worker2   &lt;none&gt;           &lt;none&gt;
virt-launcher-mssql2016z2qnw-t924b   0/2     ContainerCreating   0          16s   &lt;none&gt;   kni-worker6   &lt;none&gt;           &lt;none&gt;

<span class="o">[</span>root@eko1 ind-affinity]# kubectl get vmi <span class="nt">-o</span> wide
NAME             AGE   PHASE        IP    NODENAME   LIVE-MIGRATABLE
mssql2016p948r   34s   Scheduling
mssql2016rd4lk   34s   Scheduling
mssql2016z2qnw   34s   Scheduling

<span class="nv">$ </span>kubectl get vmi <span class="nt">-o</span> wide
NAME             AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql2016p948r   3m25s   Running   10.244.1.4   kni-worker4   False
mssql2016rd4lk   3m25s   Running   10.244.2.4   kni-worker2   False
mssql2016z2qnw   3m25s   Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<div class="premonition warning"><div class="fa fa-exclamation-circle"></div><div class="content"><p class="header">Warning</p><p><code class="highlighter-rouge">nodeSelector</code> provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature greatly expands the types of constraints you can express.</p>


</div></div>
<p>Let‚Äôs test what happens if the node running the database must be rebooted due to an upgrade or any other valid reason. First, a <a href="/2019/NodeDrain-KubeVirt.html">node drain</a> must be executed in order to evacuate all pods running and mark the node as unschedulable.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl drain kni-worker2 <span class="nt">--delete-local-data</span> <span class="nt">--ignore-daemonsets</span><span class="o">=</span><span class="nb">true</span> <span class="nt">--force</span>
node/kni-worker2 already cordoned
evicting pod <span class="s2">"virt-launcher-mssql2016rd4lk-6zz9d"</span>
pod/virt-launcher-mssql2016rd4lk-6zz9d evicted
node/kni-worker2 evicted
</code></pre></div></div>

<p>The result is an unwanted scenario, where two databases are being executed in the same high performing server. <em>This leads us to more advanced scheduling features like affinity and anti-affinity.</em></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi <span class="nt">-o</span> wide
NAME             AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql201696sz9   7m16s   Running   10.244.5.5   kni-worker6   False
mssql2016p948r   19m     Running   10.244.1.4   kni-worker4   False
mssql2016z2qnw   19m     Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>The <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">affinity/anti-affinity rules</a> solve much more complex scenarios compared to <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">nodeSelector</a>. Some of the key enhancements are:</p>

<ul>
  <li>The language is more expressive (not just ‚ÄúAND or exact match‚Äù).</li>
  <li>You can indicate that the rule is ‚Äúsoft‚Äù/‚Äùpreference‚Äù rather than a hard requirement, so if the scheduler can‚Äôt satisfy it, the pod will still be scheduled.</li>
  <li>You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located.</li>
</ul>

<p>Before going into more detail, it should be noticed that there are currently two types of <strong>affinity</strong> that applies to both <em>Node and Pod affinity</em>. They are called <code class="highlighter-rouge">requiredDuringSchedulingIgnoredDuringExecution</code> and <code class="highlighter-rouge">preferredDuringSchedulingIgnoredDuringExecution</code>. You can think of them as <em>hard</em> and <em>soft</em> respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee.</p>

<p>Said that, it is time to edit the <code class="highlighter-rouge">VirtualMachineInstanceReplicaSet</code> YAML file. Actually, <code class="highlighter-rouge">nodeSelector</code> must be removed and two different affinity rules created instead.</p>

<ol>
  <li><strong>nodeAffinity rule</strong>. This rule ensures that during scheduling time the application (MS SQL2016) must be placed <em>only</em> on nodes where the key performance contains the value high. Note the word only, there is no room for other nodes.</li>
  <li><strong>podAntiAffinity rule</strong>. This rule ensures that two applications with the key <code class="highlighter-rouge">kubevirt.io/domain</code> equals to <code class="highlighter-rouge">mssql2016</code> must not run in the same zone. Notice that the only application with this key value is the database itself and more important, notice that this rule applies to the topologyKey <code class="highlighter-rouge">topology.kubernetes.io/zone</code>. This means that only one database instance can run in each zone, e.g. one database in <em>scu, bcn and sab</em> respectively.</li>
</ol>

<p>In principle, the <code class="highlighter-rouge">topologyKey</code> can be any legal label-key. However, for performance and security reasons, there are some constraints on <code class="highlighter-rouge">topologyKey</code> that need to be taken into consideration:</p>

<ul>
  <li>For affinity and for <code class="highlighter-rouge">requiredDuringSchedulingIgnoredDuringExecution</code> pod anti-affinity, empty <code class="highlighter-rouge">topologyKey</code> is not allowed.</li>
  <li>For <code class="highlighter-rouge">preferredDuringSchedulingIgnoredDuringExecution</code> pod anti-affinity, empty <code class="highlighter-rouge">topologyKey</code> is interpreted as ‚Äúall topologies‚Äù (‚Äúall topologies‚Äù here is now limited to the combination of <code class="highlighter-rouge">kubernetes.io/hostname</code>, <code class="highlighter-rouge">topology.kubernetes.io/zone</code> and <code class="highlighter-rouge">topology.kubernetes.io/region</code>).</li>
  <li>For <code class="highlighter-rouge">requiredDuringSchedulingIgnoredDuringExecution</code> pod anti-affinity, the admission controller <code class="highlighter-rouge">LimitPodHardAntiAffinityTopology</code> was introduced to limit <code class="highlighter-rouge">topologyKey</code> to <code class="highlighter-rouge">kubernetes.io/hostname</code>. Verify if it is enabled or disabled.</li>
</ul>

<p>Here is the <code class="highlighter-rouge">VirtualMachineInstanceReplicaSet</code> object replaced.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl edit virtualmachineinstancereplicaset.kubevirt.io/mssql2016
virtualmachineinstancereplicaset.kubevirt.io/mssql2016 edited
</code></pre></div></div>

<p>Now, it contains both affinity rules:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstanceReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mssql2016replicaset</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">mssql2016</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mssql2016</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">mssql2016</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">affinity</span><span class="pi">:</span>
        <span class="na">nodeAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures the application (MS SQL2016) must ONLY be placed on nodes where the key performance contains the value high</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="na">nodeSelectorTerms</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">performance</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">high</span>
        <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that two applications with the key kubevirt.io/domain equals to mssql2016 cannot run in the same zone</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">labelSelector</span><span class="pi">:</span>
                <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">kubevirt.io/domain</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">mssql2016</span>
              <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">topology.kubernetes.io/zone</span>
      <span class="na">domain</span><span class="pi">:</span>
</code></pre></div></div>

<p>Notice that the VM or POD placement is executed only during the scheduling process, therefore we need to delete one of the <code class="highlighter-rouge">VirtualMachineInstances</code> (VMI) running in the same node. Deleting the VMI will make Kubernetes spin up a new one to reconcile the desired number of replicas (3).</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Remember to mark the kni-worker2 as schedulable again.</p>


</div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl uncordon kni-worker2
node/kni-worker2 uncordoned
</code></pre></div></div>

<p>Below shows the current status, where two databases are running in the <code class="highlighter-rouge">kni-worker6</code> node. By applying the previous affinity rules this should not happen again:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi <span class="nt">-o</span> wide
NAME             AGE   PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql201696sz9   12m   Running   10.244.5.5   kni-worker6   False
mssql2016p948r   24m   Running   10.244.1.4   kni-worker4   False
mssql2016z2qnw   24m   Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>Next, we delete one of the VMIs running in <code class="highlighter-rouge">kni-worker6</code> and wait for the rules to be applied at scheduling time. As can be seen, databases are distributed across zones and high performing nodes:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete vmi mssql201696sz9
virtualmachineinstance.kubevirt.io <span class="s2">"mssql201696sz9"</span> deleted

<span class="nv">$ </span>kubectl get vmi <span class="nt">-o</span> wide
NAME             AGE   PHASE     IP           NODENAME      LIVE-MIGRATABLE
mssql2016p948r   40m   Running   10.244.1.4   kni-worker4   False
mssql2016tpj6n   22s   Running   10.244.2.5   kni-worker2   False
mssql2016z2qnw   40m   Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>During the deployment of the clustered database <code class="highlighter-rouge">nodeAffinity</code> and <code class="highlighter-rouge">nodeSelector</code> rules were compared, however, there are a couple of things to take into consideration when creating node affinity rules, it is worth taking a look at <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">node affinity in Kubernetes documentation</a>.</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>If you remove or change the label of the node where the Pod is scheduled, the Pod will not be removed. In other words, the affinity selection works only at the time of scheduling the Pod.</p>


</div></div>
<h3 id="the-proxy-http-cache">The proxy http cache</h3>

<p>Now, that the database is configured by database administrators and running across multiple zones, it‚Äôs time to spin up the varnish http-cache container image. This time we are going to run it as a Pod instead of as a KubeVirt VM., however, scheduling rules are still valid for both objects.</p>

<p>A detailed explanation on how to run a <a href="https://varnish-cache.org/releases/index.html">Varnish Cache</a> in a Kubernetes cluster can be found in <a href="https://github.com/mittwald/kube-httpcache">kube-httpcache</a> repository. Below are detailed the steps taken:</p>

<p>Start by creating a ConfigMap that contains a VCL template and a Secret object that contains the secret for the Varnish administration port. Then apply the <a href="https://github.com/mittwald/kube-httpcache#deploy-varnish">Varnish deployment config</a>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl create <span class="nt">-f</span> configmap.yaml
configmap/vcl-template created

<span class="nv">$ </span>kubectl create secret generic varnish-secret <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">secret</span><span class="o">=</span><span class="si">$(</span><span class="nb">head</span> <span class="nt">-c32</span> /dev/urandom  | <span class="nb">base64</span><span class="si">)</span>
secret/varnish-secret created
</code></pre></div></div>

<p>In our specific mandate, 3 replicas of our web cache application are needed. Each one must be running in a different zone or datacenter. Preferably, if possible, expected to run in a Kubernetes node different from the database since as administrators we would like the database to take advantage of all the possible resources of the high-performing server. Taken into account this prerequisite, the following advanced rules are applied:</p>

<ol>
  <li><strong>nodeAffinity rule</strong>. This rule ensures that during scheduling time the application should be placed <em>if possible</em> on nodes where the key performance does not contain the value high. Note the word <em>if possible</em>. This means, it will try to run on a not performing server, however, if there none available it will be co-located with the database.</li>
  <li><strong>podAntiAffinity rule</strong>. This rule ensures that two applications with the key <code class="highlighter-rouge">app</code> equals to <code class="highlighter-rouge">cache</code> must not run in the same zone. Notice that the only application with this key value is the Varnish http-cache itself and more important, notice that this rule applies to the topologyKey <code class="highlighter-rouge">topology.kubernetes.io/zone</code>. This means that only one Varnish http-cache instance can run in each zone, e.g. one http-cache in <em>scu, bcn and sab</em> respectively.</li>
</ol>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">varnish-cache</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">cache</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">cache</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">affinity</span><span class="pi">:</span>
        <span class="na">nodeAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that during scheduling time the application must be placed *if possible* on nodes NOT performance=high</span>
          <span class="na">preferredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">weight</span><span class="pi">:</span> <span class="m">10</span>
              <span class="na">preference</span><span class="pi">:</span>
                <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">performance</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">NotIn</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">high</span>
        <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that the application cannot run in the same zone (app=cache).</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">labelSelector</span><span class="pi">:</span>
                <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">app</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">cache</span>
              <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">topology.kubernetes.io/zone</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cache</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/spaces/kube-httpcache:stable</span>
          <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">Always</span>
</code></pre></div></div>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>In this set of affinity rules, a new scheduling policy has been introduced: <code class="highlighter-rouge">preferredDuringSchedulingIgnoredDuringExecution</code>. It can be thought as ‚Äúsoft‚Äù scheduling, in the sense that it specifies preferences that the scheduler will try to enforce but will not guarantee.</p>

<p>The weight field in preferredDuringSchedulingIgnoredDuringExecution must be in the range 1-100 and it is taken into account in the <a href="#introduction">scoring step</a>. Remember that the node(s) with the highest total score is/are the most preferred.</p>


</div></div>
<p>Here, the modified deployment is applied:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@eko1 varnish]# kubectl create <span class="nt">-f</span> deployment.yaml
deployment.apps/varnish-cache created
</code></pre></div></div>

<p>The Pod is scheduled as expected since there is a node available in each zone without the <code class="highlighter-rouge">performance=high</code> label.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
varnish-cache-54489f9fc9-5pbr2       1/1     Running   0          91s   10.244.4.5   kni-worker5   &lt;none&gt;           &lt;none&gt;
varnish-cache-54489f9fc9-9s9tm       1/1     Running   0          91s   10.244.3.5   kni-worker3   &lt;none&gt;           &lt;none&gt;
varnish-cache-54489f9fc9-dflzs       1/1     Running   0          91s   10.244.6.5   kni-worker    &lt;none&gt;           &lt;none&gt;
virt-launcher-mssql2016p948r-257pn   2/2     Running   0          70m   10.244.1.4   kni-worker4   &lt;none&gt;           &lt;none&gt;
virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          31m   10.244.2.5   kni-worker2   &lt;none&gt;           &lt;none&gt;
virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          70m   10.244.5.4   kni-worker6   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>At this point, database and http-cache components of our customapp are up and running. Only the appliance created by an external provider needs to be deployed to complete the stack.</p>

<h3 id="the-third-party-appliance-virtual-machine">The third-party appliance virtual machine</h3>

<p>A third-party provider delivered a black box (appliance) in the form of a virtual machine where the application bought by the finance department is installed. Lucky to us, we have been able to transform it into a container VM ready to be run in our cluster with the help of KubeVirt.</p>

<p>Following up with our objective, this web application must take advantage of the web cache application running as a Pod. So we require the appliance to be co-located in the same server that Varnish Cache in order to accelerate the delivery of the content provided by the appliance. Also, it is required to run every replica of the appliance in different zones or data centers. Taken into account these prerequisites, the following advanced rules are configured:</p>

<ol>
  <li><strong>podAffinity rule</strong>. This rule ensures that during scheduling time the application must be placed on nodes where an application (Pod) with key <code class="highlighter-rouge">app' equals to</code>cache` is running. That is to say where the Varnish Cache is running. Note that this is mandatory, it will only run co-located with the web cache Pod.</li>
  <li><strong>podAntiAffinity rule</strong>. This rule ensures that two applications with the key <code class="highlighter-rouge">kubevirt.io/domain</code> equals to <code class="highlighter-rouge">blackbox</code> must not run in the same zone. Notice that the only application with this key value is the appliance and more important, notice that this rule applies to the topologyKey <code class="highlighter-rouge">topology.kubernetes.io/zone</code>. This means that only one appliance instance can run in each zone, e.g. one appliance in <em>scu, bcn and sab</em> respectively.</li>
</ol>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstanceReplicaSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">blackbox</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">blackbox</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">blackbox</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="s">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">blackbox</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">affinity</span><span class="pi">:</span>
        <span class="na">podAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that during scheduling time the application must be placed on nodes where Varnish Cache is running</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">labelSelector</span><span class="pi">:</span>
                <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">app</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">cache</span>
              <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">topology.kubernetes.io/hostname</span>
        <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that two applications with the key `kubevirt.io/domain` equals to `blackbox` cannot run in the same zone</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">labelSelector</span><span class="pi">:</span>
                <span class="na">matchExpressions</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kubevirt.io/domain"</span>
                    <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                    <span class="na">values</span><span class="pi">:</span>
                      <span class="pi">-</span> <span class="s">blackbox</span>
              <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">topology.kubernetes.io/zone</span>
      <span class="na">domain</span><span class="pi">:</span>
</code></pre></div></div>

<p>Here, the modified deployment is applied. As expected the VMI is scheduled as expected in the same Kubernetes nodes as Varnish Cache and each one in a different datacenter or zone.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pods,vmi <span class="nt">-o</span> wide

NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          172m    10.244.4.5   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          172m    10.244.3.5   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-dflzs	 1/1     Running   0          172m    10.244.6.5   kni-worker    &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxtk49x-nw45s    2/2     Running   0          2m31s   10.244.6.6   kni-worker    &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          2m31s   10.244.4.9   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          2m31s   10.244.3.6   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h1m    10.244.1.4   kni-worker4   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h22m   10.244.2.5   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h1m    10.244.5.4   kni-worker6   &lt;none&gt;           &lt;none&gt;

NAME                                                AGE     PHASE     IP           NODENAME	 LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackboxtk49x    2m31s   Running   10.244.6.6   kni-worker    False
virtualmachineinstance.kubevirt.io/blackboxxt829    2m31s   Running   10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    2m31s   Running   10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h1m    Running   10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h22m   Running   10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h1m    Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>At this point, our stack has been successfully deployed and configured accordingly to the requirements agreed. However, it is important before going into production to verify the proper behaviour in case of node failures. That‚Äôs what is going to be shown in the next section.</p>

<h2 id="verify-the-resiliency-of-our-customapp">Verify the resiliency of our customapp</h2>

<p>In this section, several tests must be executed to validate that the scheduling already in place is line up with the expected behaviour of the customapp application.</p>

<h3 id="draining-a-regular-node">Draining a regular node</h3>

<p>In this test, the node located in <code class="highlighter-rouge">scu</code> zone which is not labelled as high-performance will be upgraded. The proper procedure to maintain a Kubernetes node is as follows: drain the node, upgrade packages and then reboot it.</p>

<p>As it is depicted, once the <code class="highlighter-rouge">kni-worker</code> is marked as unschedulable and drained, the Varnish Cache pod and the black box appliance VM are automatically moved to the high-performance node in the same zone.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h8m    10.244.4.5   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s5sr	 1/1     Running   0          2m32s   10.244.2.7   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h8m    10.244.3.5   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxh5tg-g7hns    2/2     Running   0          13m     10.244.2.8   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          18m     10.244.4.9   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          18m     10.244.3.6   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h17m   10.244.1.4   kni-worker4   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h37m   10.244.2.5   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h17m   10.244.5.4   kni-worker6   &lt;none&gt;           &lt;none&gt;

NAME                                                AGE     PHASE     IP           NODENAME      LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackboxxh5tg    13m     Running   10.244.2.8   kni-worker2   False
virtualmachineinstance.kubevirt.io/blackboxxt829    18m     Running	 10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    18m     Running	 10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h17m   Running	 10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h37m   Running	 10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h17m   Running	 10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>Remember that this is happening because:</p>

<ul>
  <li>There is a <strong>mandatory</strong> policy that only one replica of each application can run at the same time in the same zone.</li>
  <li>There is a <strong>soft policy</strong> (preferred) that both applications should run on a non high-performance node. However, since there are any of these nodes available it has been scheduled in the high-performance server along with the database.</li>
  <li>Both applications must run in the same node</li>
</ul>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Note that uncordoning the node will not make the blackbox appliance and the Varnish Cache pod to come back to the previous node.</p>


</div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl uncordon kni-worker
node/kni-worker uncordoned

NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h10m   10.244.4.5   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s5sr	 1/1     Running   0          5m29s   10.244.2.7   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h10m   10.244.3.5   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxh5tg-g7hns    2/2     Running   0          16m     10.244.2.8   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          21m     10.244.4.9   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          21m     10.244.3.6   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h20m   10.244.1.4   kni-worker4   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h40m   10.244.2.5   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h20m   10.244.5.4   kni-worker6   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>In order to return to the most desirable state, the pod and VM from kni-worker2 must be deleted.</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Both applications must be deleted since the <code class="highlighter-rouge">requiredDuringSchedulingIgnoredDuringExecution</code> policy is only applied during scheduling time.</p>


</div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl delete  pod/varnish-cache-54489f9fc9-9s5sr
pod <span class="s2">"varnish-cache-54489f9fc9-9s5sr"</span> deleted

<span class="nv">$ </span>kubectl delete virtualmachineinstance.kubevirt.io/blackboxxh5tg
virtualmachineinstance.kubevirt.io <span class="s2">"blackboxxh5tg"</span> deleted
</code></pre></div></div>

<p>Once done, the scheduling process is run again for both applications and the applications are placed in the most desirable node taking into account affinity rules configured.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                     READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
pod/varnish-cache-54489f9fc9-5pbr2	 1/1     Running   0          3h13m   10.244.4.5   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-9s9tm	 1/1     Running   0          3h13m   10.244.3.5   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/varnish-cache-54489f9fc9-fldhc	 1/1     Running   0          2m7s    10.244.6.7   kni-worker    &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackbox54l7t-4c6wh    2/2     Running   0          23s     10.244.6.8   kni-worker    &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxxt829-snjth    2/2     Running   0          23m     10.244.4.9   kni-worker5   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-blackboxzf9kt-6mh56    2/2     Running   0          23m     10.244.3.6   kni-worker3   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016p948r-257pn   2/2     Running   0          4h23m   10.244.1.4   kni-worker4   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016tpj6n-l2fnf   2/2     Running   0          3h43m   10.244.2.5   kni-worker2   &lt;none&gt;           &lt;none&gt;
pod/virt-launcher-mssql2016z2qnw-t924b   2/2     Running   0          4h23m   10.244.5.4   kni-worker6   &lt;none&gt;           &lt;none&gt;

NAME                                                AGE     PHASE     IP           NODENAME	 LIVE-MIGRATABLE
virtualmachineinstance.kubevirt.io/blackbox54l7t    23s     Running   10.244.6.8   kni-worker    False
virtualmachineinstance.kubevirt.io/blackboxxt829    23m     Running   10.244.4.9   kni-worker5   False
virtualmachineinstance.kubevirt.io/blackboxzf9kt    23m     Running   10.244.3.6   kni-worker3   False
virtualmachineinstance.kubevirt.io/mssql2016p948r   4h23m   Running   10.244.1.4   kni-worker4   False
virtualmachineinstance.kubevirt.io/mssql2016tpj6n   3h43m   Running   10.244.2.5   kni-worker2   False
virtualmachineinstance.kubevirt.io/mssql2016z2qnw   4h23m   Running   10.244.5.4   kni-worker6   False
</code></pre></div></div>

<p>This behaviour can be extrapolated to a failure or shutdown of any odd or non high-performance worker node. In that case, all workloads will be moved to the high performing server <em>in the same zone</em>. Although this is not ideal, our <code class="highlighter-rouge">customapp</code> will be still available while the node recovery is ongoing.</p>

<h3 id="draining-a-high-performance-node">Draining a high-performance node</h3>

<p>On the other hand, in case of a high-performance worker node failure, which was shown <a href="#the-clustered-database">previously</a>, the database will not be able to move to another server, since there is only one high performing server per zone. A possible solution is just adding a stand-by high-performance node in each zone.</p>

<p>However, since the database is configured as a clustered database, the application running in the same zone as the failed database will still be able to establish a connection to any of the other two running databases located in different zones. This configuration is done at the application level. Actually, from the application standpoint, it just connects to a database pool of resources.</p>

<p>Since this is not ideal either, e.g. establishing a connection to another zone or datacenter takes longer than in the same datacenter, the application will be still available and providing service to the clients.</p>

<h2 id="affinity-rules-are-everywhere">Affinity rules are everywhere</h2>

<p>As written in the title section, affinity rules are essential to provide high availability and resiliency to Kubernetes applications. Furthermore, KubeVirt‚Äôs components also take advantage of these rules to avoid unwanted situations that could compromise the stability of the VMs running in the cluster.</p>

<p>For instance, below it is partly shown a snippet of the deployment object for virt-api and virt-controller. Notice the following affinity rule created:</p>

<ol>
  <li><strong>podAntiAffinity rule</strong>. This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node (<code class="highlighter-rouge">kubernetes.io/hostname</code>). It is used the key <code class="highlighter-rouge">kubevirt.io</code> to match the application <code class="highlighter-rouge">virt-api</code> or <code class="highlighter-rouge">virt-controller</code>. See that it is a soft requirement, which means that the kube-scheduler will try to match the rule, however, if it is not possible it can place both replicas in the same node.</li>
</ol>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
        <span class="na">metadata</span><span class="pi">:</span>
          <span class="na">labels</span><span class="pi">:</span>
            <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-api</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">virt-api</span>
          <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt</span>
        <span class="na">spec</span><span class="pi">:</span>
          <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
          <span class="na">selector</span><span class="pi">:</span>
            <span class="na">matchLabels</span><span class="pi">:</span>
              <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-api</span>
          <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">template</span><span class="pi">:</span>
            <span class="na">metadata</span><span class="pi">:</span>
              <span class="na">annotations</span><span class="pi">:</span>
                <span class="s">scheduler.alpha.kubernetes.io/critical-pod</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
                <span class="s">scheduler.alpha.kubernetes.io/tolerations</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"key":"CriticalAddonsOnly","operator":"Exists"}]'</span>
              <span class="na">labels</span><span class="pi">:</span>
                <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-api</span>
                <span class="s">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">virt-api</span>
            <span class="na">spec</span><span class="pi">:</span>
              <span class="na">affinity</span><span class="pi">:</span>
                <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node</span>
                  <span class="na">preferredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="na">podAffinityTerm</span><span class="pi">:</span>
                      <span class="na">labelSelector</span><span class="pi">:</span>
                        <span class="na">matchExpressions</span><span class="pi">:</span>
                        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">kubevirt.io</span>
                          <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                          <span class="na">values</span><span class="pi">:</span>
                          <span class="pi">-</span> <span class="s">virt-api</span>
                      <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">kubernetes.io/hostname</span>
                    <span class="na">weight</span><span class="pi">:</span> <span class="m">1</span>
              <span class="na">containers</span><span class="pi">:</span>
              <span class="s">...</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-controller</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virt-controller</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-controller</span>
  <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="s">scheduler.alpha.kubernetes.io/critical-pod</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="s">scheduler.alpha.kubernetes.io/tolerations</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"key":"CriticalAddonsOnly","operator":"Exists"}]'</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-controller</span>
        <span class="s">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">virt-controller</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">affinity</span><span class="pi">:</span>
        <span class="na">podAntiAffinity</span><span class="pi">:</span> <span class="c1">#This rule ensures that two replicas of the same application should not run if possible in the same Kubernetes node</span>
          <span class="na">preferredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">podAffinityTerm</span><span class="pi">:</span>
                <span class="na">labelSelector</span><span class="pi">:</span>
                  <span class="na">matchExpressions</span><span class="pi">:</span>
                    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">kubevirt.io</span>
                      <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                      <span class="na">values</span><span class="pi">:</span>
                        <span class="pi">-</span> <span class="s">virt-controller</span>
                <span class="na">topologyKey</span><span class="pi">:</span> <span class="s">kubernetes.io/hostname</span>
              <span class="na">weight</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>It is worth mentioning that DaemonSets internally also uses advanced scheduling rules. Basically, they are <code class="highlighter-rouge">nodeAffinity</code> rules in order to place each replica in each Kubernetes node of the cluster.</p>


</div></div>
<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</p>


</div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>root@eko1 varnish]# kubectl get daemonset <span class="nt">-n</span> kubevirt
NAMESPACE     NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
kubevirt      virt-handler   6         6         6       6            6           &lt;none&gt;                        25h
</code></pre></div></div>

<p>See the partial snippet of a <code class="highlighter-rouge">virt-handler</code> Pod created by a DaemonSet (see ownerReferences section, kind: DaemonSet) that configures a <code class="highlighter-rouge">nodeAffinity</code> rule that requires the Pod to run in a specific hostname matched by the key <code class="highlighter-rouge">metadata.name</code> and value the name of the node (<code class="highlighter-rouge">kni-worker2</code>). Note that the value of the key changes depending on the nodes that are part of the cluster, this is done by the DaemonSet.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">kubevirt.io/install-strategy-identifier</span><span class="pi">:</span> <span class="s">0000ee7f7cd4756bb221037885c3c86816db6de7</span>
    <span class="s">kubevirt.io/install-strategy-registry</span><span class="pi">:</span> <span class="s">index.docker.io/kubevirt</span>
    <span class="s">kubevirt.io/install-strategy-version</span><span class="pi">:</span> <span class="s">v0.26.0</span>
    <span class="s">scheduler.alpha.kubernetes.io/critical-pod</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
    <span class="s">scheduler.alpha.kubernetes.io/tolerations</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"key":"CriticalAddonsOnly","operator":"Exists"}]'</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2020-02-12T11:11:14Z"</span>
  <span class="na">generateName</span><span class="pi">:</span> <span class="s">virt-handler-</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">app.kubernetes.io/managed-by</span><span class="pi">:</span> <span class="s">kubevirt-operator</span>
    <span class="na">controller-revision-hash</span><span class="pi">:</span> <span class="s">84d96d4775</span>
    <span class="s">kubevirt.io</span><span class="pi">:</span> <span class="s">virt-handler</span>
    <span class="na">pod-template-generation</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
    <span class="s">prometheus.kubevirt.io</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virt-handler-ctzcg</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt</span>
  <span class="na">ownerReferences</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
      <span class="na">blockOwnerDeletion</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">controller</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">DaemonSet</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">virt-handler</span>
      <span class="na">uid</span><span class="pi">:</span> <span class="s">6e7faece-a7aa-4ed0-959e-4332b2be4ec3</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">28301"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/kubevirt/pods/virt-handler-ctzcg</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">95d68dad-ad06-489f-b3d3-92413bcae1da</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">affinity</span><span class="pi">:</span>
    <span class="na">nodeAffinity</span><span class="pi">:</span>
      <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
        <span class="na">nodeSelectorTerms</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">matchFields</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">metadata.name</span>
                <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                <span class="na">values</span><span class="pi">:</span>
                  <span class="pi">-</span> <span class="s">kni-worker2</span>
</code></pre></div></div>

<h2 id="summary">Summary</h2>

<p>In this blog post, a real use case has been detailed on how advanced scheduling can be configured in a hybrid scenario where VMs and Pods are part of the same application stack. The reader can realize that Kubernetes itself already provides a lot of functionality out of the box to Pods running on top of it. One of these inherited capabilities is the possibility to create even more complex affinity or/and anti-affinity rules than traditional virtualization products.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/">Kubernetes labels and annotations official documentation</a>]</li>
  <li><a href="https://kubevirt.io/2019/NodeDrain-KubeVirt.html">Kubevirt node drain blog post</a></li>
  <li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">Kubernetes node affinity documentation</a>.</li>
  <li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md">Kubernetes design proposal for Inter-pod topological affinity and anti-affinity</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt/pull/2089">KubeVirt add affinity to virt pods pull request discussion</a></li>
</ul>
:ET