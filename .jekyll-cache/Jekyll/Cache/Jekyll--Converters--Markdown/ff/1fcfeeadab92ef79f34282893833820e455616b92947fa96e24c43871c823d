I"’<p>Gluster seems like a good fit for storage in kubernetes and in particular in kubevirt. Still, as for other storage backends, we will likely need to use a golden set of images and deploy vms from them.</p>

<p>Thatâ€™s where cloning feature of gluster comes at rescue!</p>

<h2 id="contents">Contents</h2>

<ul>
  <li>Prerequisites</li>
  <li>Installing Gluster provisioner</li>
  <li>Using The cloning feature</li>
  <li>Conclusion</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<p>I assume you already have a running instance of openshift and kubevirt along with gluster and an already existing pvc where you copied a base operating system ( you can get those from <a href="https://docs.openstack.org/image-guide/obtain-images.html">here</a>)</p>

<p>For reference, I used the following components and versions:</p>

<ul>
  <li>3 baremetal servers with Rhel 7.4 as base OS</li>
  <li>OpenShift and CNS 3.9</li>
  <li>KubeVirt latest</li>
</ul>

<h2 id="installing-gluster-provisioner">Installing Gluster provisioner</h2>

<h3 id="initial-deployment">initial deployment</h3>

<p>We will deploy the custom provisioner using <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/glusterfile-provisioner-template.yml">this template</a>, along with cluster rules located in <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/openshift-clusterrole.yaml">this file</a></p>

<p>Note that we also patch the image to use an existing one from gluster org located at docker.io instead of quay.io, as the corresponding repository is private by the time of this writing, and the heketi one, to make sure it has the code required to handle cloning</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE="app-storage"
oc create -f openshift-clusterrole.yaml
oc process -f glusterfile-provisioner-template.yml | oc apply -f - -n $NAMESPACE
oc adm policy add-cluster-role-to-user cluster-admin -z glusterfile-provisioner -n $NAMESPACE
oc adm policy add-scc-to-user privileged -z glusterfile-provisioner
oc set image dc/heketi-storage heketi=gluster/heketiclone:latest  -n $NAMESPACE
oc set image dc/glusterfile-provisioner glusterfile-provisioner=gluster/glusterfileclone:latest  -n $NAMESPACE
</code></pre></div></div>

<p>And you will see something similar to this in your storage namespace</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@master01 ~]# NAMESPACE="app-storage"
[root@master01 ~]# kubectl get pods -n $NAMESPACE
NAME                              READY     STATUS    RESTARTS   AGE
glusterfile-provisioner-3-vhkx6   1/1       Running   0          1d
glusterfs-storage-b82x4           1/1       Running   1          23d
glusterfs-storage-czthc           1/1       Running   0          23d
glusterfs-storage-z68hm           1/1       Running   0          23d
heketi-storage-2-qdrks            1/1       Running   0          6h
</code></pre></div></div>

<h3 id="additional-configuration">additional configuration</h3>

<p>for the custom provisioner to work, we need two additional things:</p>

<ul>
  <li>a storage class pointing to it, but also containing the details of the current heketi installation</li>
  <li>a secret similar to the one used by the current heketi installation, but using a different <em>type</em></li>
</ul>

<p>You can use the following</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAMESPACE="app-storage"
oc get sc glusterfs-storage -o yaml
oc get secret heketi-storage-admin-secret -n $NAMESPACE-o yaml
</code></pre></div></div>

<p>then, create the following objects:</p>

<ul>
  <li>glustercloning-heketi-secret secret in your storage namespace</li>
  <li>glustercloning storage class</li>
</ul>

<p>for reference, here are samples of those files.</p>

<p>Note how we change the type for the secret and add extra options for our storage class (in particular, enabling smartclone).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
data:
  key: eEt0NUJ4cklPSmpJb2RZcFpqVExSSjUveFV5WHI4L0NxcEtMME1WVlVjQT0=
kind: Secret
metadata:
  name: glustercloning-heketi-secret
  namespace: app-storage
type: gluster.org/glusterfile
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: glustercloning
parameters:
  restsecretname: glustercloning-heketi-secret
  restsecretnamespace: app-storage
  resturl: http://heketi-storage.192.168.122.10.xip.io
  restuser: admin
  smartclone: "true"
  snapfactor: "10"
  volumeoptions: group virt
provisioner: gluster.org/glusterfile
reclaimPolicy: Delete
</code></pre></div></div>

<p>The full set of supported parameters can be found <a href="https://github.com/kubernetes-incubator/external-storage/blob/master/gluster/file/README.md">here</a></p>

<h2 id="using-the-cloning-feature">Using the cloning feature</h2>

<p>Once deployed, you can now provision pvcs from a base origin</p>

<h3 id="cloning-single-pvcs">Cloning single pvcs</h3>

<p>For instance, provided you have an existing pvc named <em>cirros</em> containing this base operating system, and that this PVC contains an annotion of the following</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(...)
metadata:
 annotations:
  gluster.org/heketi-volume-id: f0cbbb29ef4202c5226f87708da57e5c
(...)
</code></pre></div></div>

<p>A cloned pvc can be created with the following yaml ( note that we simply indicate a clone request in the annotations)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testclone1
  namespace: default
  annotations:
    k8s.io/CloneRequest: cirros
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: glustercloning
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
</code></pre></div></div>

<p>Once provisioned, the pvc will contain this additional annotation created by the provisioner</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(...)
metadata:
 annotations:
      k8s.io/CloneOf: cirros

(...)
</code></pre></div></div>

<h3 id="leveraging-the-feature-in-openshift-templates">Leveraging the feature in openshift templates</h3>

<p>We can make direct use of the feature in <a href="../assets/2018-05-16-use-glustercloning-with-kubevirt/template.yml">this openshift template</a> which would create the following objects:</p>

<ul>
  <li>a persistent volume claim as a clone of an existing pvc (cirros by default)</li>
  <li>an offline virtual machine object</li>
  <li>additional services for ssh and http access</li>
</ul>

<p>you can use it with something like</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>oc process -f template.yml -p Name=myvm | oc process -f - -n default
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Cloning features in the storage backend allow us to simply use a given set of pvcs as base OS for the deployment of our vms. This feature is growing in gluster, worth giving it a try!</p>
:ET