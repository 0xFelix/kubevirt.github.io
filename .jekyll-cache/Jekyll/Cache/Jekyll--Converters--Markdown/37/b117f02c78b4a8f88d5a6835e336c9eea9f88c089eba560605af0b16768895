I"Ý#<h2 id="introduction">Introduction</h2>

<p>In the article <a href="/2019/jenkins-jobs-for-kubevirt-lab-validation.html">Jenkins Jobs for KubeVirt Lab Validation</a>, we covered how Jenkins did get the information about the labs and jobs to perform from the KubeVirt repositories.</p>

<p>In this article, weâ€™ll cover the configuration changes in both Jenkins and the JenkinsFiles required to get our CI setup updated to latest versions and syntax .</p>

<h2 id="jenkins">Jenkins</h2>

<p>Our Jenkins instance, is running on top of <a href="https://console.apps.ci.centos.org:8443/console/">CentOS OpenShift</a> and is one of the OS-enhanced Jenkins instances, that provide persistent storage and other pieces bundled, required for non-testing setups.</p>

<p>What we found is that Jenkins was already complaining because of pending updates (security, engine, etc), but the <code class="highlighter-rouge">jenkins.war</code> was embedded in the container image we were using.</p>

<p>Initial attempts tried to use environment variables to override the WAR to use, but our image was not prepared for it, so we were given the option to just generate a new container for it, but this seemed a bad approach as our image, also contained custom libraries (<code class="highlighter-rouge">contra-lib</code>) that enables communicating with OpenShift to run the tests inside containers there.</p>

<p>During the investigation and testing, we found that the persistent storage folder we were using (<code class="highlighter-rouge">/var/lib/jenkins</code>) contained a <code class="highlighter-rouge">war</code> folder which contained the unarchived <code class="highlighter-rouge">jenkins.war</code>, so the next attempt was to manually download the latest <code class="highlighter-rouge">jenkins.war</code>, and unzip it on that folder, which finally allowed us to upgrade Jenkins core.</p>

<h2 id="the-plugins">The plugins</h2>

<p>After upgrading the Jenkins core, we could use the internal plugin manager to upgrade all the remaining plugins, however, that meant a big change in the plugins, configurations, etc.</p>

<p>After initially being able to run the lab validations for a while, on next morning we wanted to release a new image (from Cloud-Image-Builder) and it failed to build because of the external libraries, and also affected the lab validations again, so we got back to square one for the upgrade process, leaving us with the decision to go forward with the full upgrade: the latest stable jenkins and available plugins and reconfigure what was changed to suit the upgraded requirements.</p>

<p>Here weâ€™ll show you the configuration settings for each one of the new/updated plugins.</p>

<h3 id="openshift-plugin">OpenShift Plugin</h3>

<p>Updated to configure the instance of CentOS OpenShift with the system account for accessing it:</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-44-56.png" alt="Jenkins OpenShift Client configuration" /></p>

<h3 id="openshift-jenkins-sync">OpenShift Jenkins Sync</h3>

<p>Updated and configured to use the console as well with <code class="highlighter-rouge">kubevirt</code> namespace:</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-46-47.png" alt="Jenkins OpenShift sync configuration" /></p>

<h3 id="global-pipeline-libraries">Global Pipeline Libraries</h3>

<p>Here we added the libraries we used, but instead of a specific commit, targetting the <code class="highlighter-rouge">master</code> branch.</p>

<ul>
  <li>contra-lib: <a href="https://github.com/openshift/contra-lib.git">https://github.com/openshift/contra-lib.git</a></li>
  <li>cico-pipeline-library: <a href="https://github.com/CentOS/cico-pipeline-library.git">https://github.com/CentOS/cico-pipeline-library.git</a></li>
  <li>contra-library: <a href="https://github.com/CentOS-PaaS-SIG/contra-env-sample-project">https://github.com/CentOS-PaaS-SIG/contra-env-sample-project</a></li>
</ul>

<p>For all of them, we ticked:</p>

<ul>
  <li><code class="highlighter-rouge">Load Implicitly</code></li>
  <li><code class="highlighter-rouge">Allow default version to be overriden</code></li>
  <li><code class="highlighter-rouge">Include @Library changes in job recent changes</code></li>
</ul>

<p>Jenkins replied with the â€˜currently maps to revision: <code class="highlighter-rouge">hash</code>â€™ for each one of them, after having loaded them properly, indicating that it was successful.</p>

<h3 id="slack-plugin">Slack plugin</h3>

<p>In addition to regular plugins used for builds, we incorporated the slack plugin to validate the notifications of build status to a test slack channel. Configuration is really easy, from within Slack, when added the jenkins notifications plugin, a <code class="highlighter-rouge">token</code> is provided that must be configured in Jenkins as well as a default <code class="highlighter-rouge">room</code> to send notifications to.</p>

<p>This allows us to get notified when a new build is started and the resulting status, just in case something was generated with errors or something external changed (remember that we use latest KubeVirt release, latest tools for virtctl, kubectl and a new image is generated out of them to validate the labs).</p>

<h3 id="kubernetes-cloud">Kubernetes â€˜Cloudâ€™</h3>

<p>In addition, Kubernets <code class="highlighter-rouge">Cloud</code> was configured pointing to the same console access and using the <code class="highlighter-rouge">kubevirt</code> namespace:</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-51-19.png" alt="OpenShift Cloud definition and URL and tunnel settings" /></p>

<p>The libraries we added, automatically add some pod templates for <code class="highlighter-rouge">jenkins-contra-slave</code>:</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-54-24.png" alt="Jenkins-contra-slave container configuration" /></p>

<h2 id="other-changes">Other changes</h2>

<p>Our environment also used other helper tools as regular OpenShift builds (contra):</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-09-55-41.png" alt="OpenShift Build repositories and status" /></p>

<p>We had to update the repositories from using some older forks (no longer valid and outdated) to use the latest versions, and for the ansible-executor we also created a fork to use the newest libraries for accessing Google Cloud environment and tuning some other variables (<a href="https://github.com/CentOS-PaaS-SIG/contra-env-infra/pull/59">https://github.com/CentOS-PaaS-SIG/contra-env-infra/pull/59</a>) (changes have now landed the upstream repo).</p>

<p>The issue that we were facing was related with the failure to write temporary files to the <code class="highlighter-rouge">$HOME</code> folder for the user so ansible configuration was forced to use one in a temporary and writable folder.</p>

<p>Additionally, Google Cloud access required updating libraries for authentication that were failing as well, that is fixed via the Dockerfile that generated <code class="highlighter-rouge">ansible-executor</code> container image.</p>

<h2 id="job-changes">Job Changes</h2>

<p>Our Jenkins Jobs were defined (as documented in prior article) inside each repository that made that part easy on one side, but also required some tuning and changes:</p>

<ul>
  <li>We have disabled minikube validation as it was failing for both AWS and GCP unless using baremetal (so weâ€™re wondering about using another approach here)</li>
  <li>Weâ€™ve added code to do the actual â€˜Slackâ€™ notification we mentioned above</li>
  <li>Extend the try/catch block to include a â€˜finallyâ€™ to send the notifications</li>
  <li>Change the syntax for â€˜artifactsâ€™ as it was previously <code class="highlighter-rouge">ArchiveArtifacts</code> and now itâ€™s a <code class="highlighter-rouge">postBuild</code></li>
</ul>

<h2 id="the-outcome">The outcome</h2>

<p>After several attempts for fine-tuning the configuration, the builds started succeeding:</p>

<p><img src="/assets/2019-11-22-jenkins-ci-server-upgrade-and-jobs-for-kubevirt/2019-11-11-11-02-34.png" alt="Sunny build status" /></p>

<p>Of course, one of the advantages is that builds happen automatically every day or on code changes on the repositories.</p>

<p>Thereâ€™s still room for improvement identified that will happen in next iterations:</p>

<ul>
  <li>Find not needed running instances on Cloud providers for reducing the bills</li>
  <li>Trigger builds when new releases of KubeVirt happen (out of kubevirt/kubevirt repo)</li>
  <li>Unify testing on <a href="/2019/prow-jobs-for-kubevirt.html">Prow instance</a></li>
</ul>

<p>Of course, the builds can be failing for external reasons (like VM in cloud provider taking longer to start up and have SSH available, or the nested VM inside after importing, etc), but still a great help for checking if things are working as they should and of course, to get the validations improved for reduce the number of false positives.</p>
:ET