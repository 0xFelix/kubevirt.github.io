I"˜,<p>Building your environment for testing or automation purposes can be difficult when using different technologies. In this guide you‚Äôll find how to set up your system step-by-step to work with the latest versions of Kubernetes (up to today), CRI-O and KubeVirt.</p>

<p>In this series of blogposts the following topics are going to be covered en each post:</p>

<ul>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch.html">Requirements: dependencies and containers runtime</a></li>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_kubernetes.html">Kubernetes: Cluster and Network</a></li>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_KubeVirt.html">KubeVirt: requirements and first Virtual Machine</a></li>
</ul>

<p>In the first blogpost of the series (<a href="/2019/KubeVirt_k8s_crio_from_scratch.html">KubeVirt on Kubernetes with CRI-O from scratch)</a> the initial set up for a CRI-O runtime environment has been covered. In this post is shown the installation and configuration of Kubernetes based in the previous CRI-O environment.</p>

<h2 id="installing-kubernetes">Installing Kubernetes</h2>

<p>If the ansible way was chosen, you may want to skip this section since the repository and needed packages were already installed during execution.</p>

<p>To install the K8s packages a new repo has to be added:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">k8s-test.local</span><span class="c"># vim /etc/yum.repos.d/kubernetes.repo
</span><span class="nn">[Kubernetes]</span>
<span class="py">name</span><span class="p">=</span><span class="s">Kubernetes</span>
<span class="py">baseurl</span><span class="p">=</span><span class="s">https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span>
<span class="py">enabled</span><span class="p">=</span><span class="s">1</span>
<span class="py">gpgcheck</span><span class="p">=</span><span class="s">1</span>
<span class="py">repo_gpgcheck</span><span class="p">=</span><span class="s">1</span>
<span class="py">gpgkey</span><span class="p">=</span><span class="s">https://packages.cloud.google.com/yum/doc/yum-key.gpg</span>
<span class="err">https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span>
</code></pre></div></div>

<p>Now, the gpg keys of the packages can be imported into the system and the installation can proceed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# rpm <span class="nt">--import</span> https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg

k8s-test.local# yum <span class="nb">install</span> <span class="nt">-y</span> kubelet kubeadm kubectl
</code></pre></div></div>

<p>Once the Kubelet is configured and CRI-O also ready, the CRI-O daemon can be started and the setup of the cluster can be done:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The kubelet will not start successfully until the Kubernetes cluster is installed.</p>


</div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# systemctl restart crio

k8s-test.local# systemctl <span class="nb">enable</span> <span class="nt">--now</span> kubelet
</code></pre></div></div>

<h2 id="installing-the-kubernetes-cluster">Installing the Kubernetes cluster</h2>

<p>There are multiple ways for installing a Kubernetes cluster, in this example it will be done with the command <code class="highlighter-rouge">kubeadm</code>, the pod network cidr is the same that has been previously used for the CRI-O bridge in the <code class="highlighter-rouge">10-crio-bridge.conf</code> configuration file:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# kubeadm init <span class="nt">--pod-network-cidr</span><span class="o">=</span>10.244.0.0/16
</code></pre></div></div>

<p>When the installation finishes the command will print a similar message like this one:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.10:6443 --token 6fsrbi.iqsw1girupbwue5o \
    --discovery-token-ca-cert-hash sha256:c7cf9d9681876856f9b7819067841436831f19004caadab0b5838a9bf7f4126a
</code></pre></div></div>

<p>Now, it‚Äôs time to deploy the pod network. If the reader is curious and want to already check the status of the cluster, the following commands can be executed for getting all the pods running and their status:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# <span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>/etc/kubernetes/kubelet.conf

k8s-test.local# kubectl get pods <span class="nt">-A</span>
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-ffnvx           1/1     Running   0          101s
kube-system   coredns-5644d7b6d9-lh9gm           1/1     Running   0          101s
kube-system   etcd-k8s-test                      1/1     Running   0          59s
kube-system   kube-apiserver-k8s-test            1/1     Running   0          54s
kube-system   kube-controller-manager-k8s-test   1/1     Running   0          58s
kube-system   kube-proxy-tdcdv                   1/1     Running   0          101s
kube-system   kube-scheduler-k8s-test            1/1     Running   0          50s
</code></pre></div></div>

<h2 id="installing-the-pod-network">Installing the pod network</h2>

<p>The <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network">Kubernetes pod-network documentation</a> shows different add-on to handle the communications between the pods.</p>

<p>In this example Virtual Machines will be deployed with KubeVirt and also they will have multiple network interfaces attached to the VMs, in this example <a href="https://github.com/intel/multus-cni">Multus</a> is going to be used.</p>

<p>Some of the <a href="https://github.com/intel/multus-cni/blob/master/doc/quickstart.md">Multus Prerequisites</a> indicate:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>After installing Kubernetes, you must install a default network CNI plugin. If you‚Äôre using kubeadm, refer to the ‚ÄúInstalling a pod network add-on‚Äù section in the kubeadm documentation. If it‚Äôs your first time, we generally recommend using Flannel for the sake of simplicity.</p>


</div></div>
<p>So flannel is going to be installed running the following commands:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# <span class="nb">cd</span> /root
k8s-test.local# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre></div></div>

<p>The version of CNI has to be checked and ensured that is the <code class="highlighter-rouge">0.3.1</code> version, otherwise, it has to be changed, in this example the version <code class="highlighter-rouge">0.2.0</code> is replaced by the <code class="highlighter-rouge">0.3.1</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# <span class="nb">grep </span>cniVersion kube-flannel.yml
      <span class="s2">"cniVersion"</span>: <span class="s2">"0.2.0"</span>,

k8s-test.local# <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/0.2.0/0.3.1/g'</span> kube-flannel.yml

k8s-test.local# kubectl apply <span class="nt">-f</span> kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
</code></pre></div></div>

<p>Once the flannel network has been created the Multus can be defined, to check the status of the pods the following command can be executed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# kubectl get pods <span class="nt">-A</span>
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-9mfc9           1/1     Running   0          20h
kube-system   coredns-5644d7b6d9-sd6ck           1/1     Running   0          20h
kube-system   etcd-k8s-test                      1/1     Running   0          20h
kube-system   kube-apiserver-k8s-test            1/1     Running   0          20h
kube-system   kube-controller-manager-k8s-test   1/1     Running   0          20h
kube-system   kube-flannel-ds-amd64-ml68d        1/1     Running   0          20h
kube-system   kube-proxy-lqjpv                   1/1     Running   0          20h
kube-system   kube-scheduler-k8s-test            1/1     Running   0          20h
</code></pre></div></div>

<p>To load the multus configuration, the <code class="highlighter-rouge">multus-cni</code> repository has to be cloned, and also the <code class="highlighter-rouge">kube-1.16-change</code> branch has to be used:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# git clone https://github.com/intel/multus-cni /root/src/github.com/multus-cni

k8s-test.local# <span class="nb">cd</span> /root/src/github.com/multus-cni

k8s-test.local# git checkout origin/kube-1.16-change

k8s-test.local# <span class="nb">cd </span>multus-cni/images
</code></pre></div></div>

<p>To load the multus daemonset the following command has to be executed:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# kubectl create <span class="nt">-f</span> multus-daemonset-crio.yml
customresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io created
clusterrole.rbac.authorization.k8s.io/multus created
clusterrolebinding.rbac.authorization.k8s.io/multus created
serviceaccount/multus created
configmap/multus-cni-config created
daemonset.apps/kube-multus-ds-amd64 created
daemonset.apps/kube-multus-ds-ppc64le created
</code></pre></div></div>

<p>In the next post <a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_KubeVirt.html">KubeVirt: requirements and first Virtual Machine</a>, the KubeVirt requirements will be set up together with the binaries and YAML files and also the first virtual Machines will be deployed.</p>
:ET