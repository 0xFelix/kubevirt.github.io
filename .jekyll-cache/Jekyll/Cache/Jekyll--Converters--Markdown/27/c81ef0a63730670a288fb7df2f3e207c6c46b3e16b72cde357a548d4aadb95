I"-?<p>Building your environment for testing or automation purposes can be difficult when using different technologies. In this guide you’ll find how to set up your system step-by-step to work with the latest versions up to today of Kubernetes, CRI-O and KubeVirt.
In this series of blogposts the following topics are going to be covered en each post:</p>

<ul>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch.html">Requirements: dependencies and containers runtime</a></li>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_kubernetes.html">Kubernetes: Cluster and Network</a></li>
  <li><a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_KubeVirt.html">KubeVirt: requirements and first Virtual Machine</a></li>
</ul>

<h2 id="pre-requisites">Pre-requisites</h2>

<h3 id="versions">Versions</h3>

<p>The following versions are going to be used:</p>

<table>
  <thead>
    <tr>
      <th>Software</th>
      <th>Purpose</th>
      <th style="text-align: center">Version</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CentOS</td>
      <td>Operating System</td>
      <td style="text-align: center">7.7.1908</td>
    </tr>
    <tr>
      <td>Kubernetes</td>
      <td>Orchestration</td>
      <td style="text-align: center">v1.16.0</td>
    </tr>
    <tr>
      <td>CRI-O</td>
      <td>Containers runtime</td>
      <td style="text-align: center">1.16.0-dev</td>
    </tr>
    <tr>
      <td>KubeVirt</td>
      <td>Virtual Machine Management on Kubernetes</td>
      <td style="text-align: center">v0.20.7</td>
    </tr>
    <tr>
      <td>Ansible (optional)</td>
      <td>Automation tool</td>
      <td style="text-align: center">2.8.4</td>
    </tr>
  </tbody>
</table>

<h3 id="requirements">Requirements</h3>

<p>It is a requirement to have a Virtual Machine (VM) with enough resources, in my case I am running a 16GB memory and 4vCPUs VM, but should probably be run with less resources. Operating System (OS) running this VM as indicated in the table above has to be CentOS 7.7.1908 and you should take care of its deployment. In my lab I used latest <a href="https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2">Centos 7 cloud image</a> to speed up the provisioning process.</p>

<p>In this guide the system will be named k8s-test.local and the IP address is 192.168.0.10. A second system called laptop would be used to run the playbooks (if you choose to go the easy and automated way). It is also needed to have access to the root account in the VM for installing the required software and configure some kernel parameters. In this example only a Kubernetes master would be used.</p>

<h2 id="instructions">Instructions</h2>

<h3 id="preparing-the-vm">Preparing the VM</h3>

<p>Ensure the VM system is updated to the latest versions of the software and also ensure that the epel repository is installed:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# yum install epel-release -y

k8s-test.local# yum update -y

k8s-test.local# yum install vim jq -y
</code></pre></div></div>

<p>The following kernel parameters have to be configured:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
</code></pre></div></div>

<p>And also the following kernel modules have to be installed:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# modprobe br_netfilter
k8s-test.local# echo br_netfilter &gt; /etc/modules-load.d/br_netfilter.conf

k8s-test.local# modprobe overlay
k8s-test.local# echo overlay &gt; /etc/modules-load.d/overlay.conf
</code></pre></div></div>

<p>The new sysctl parameters have to be loaded in the system with the following command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# sysctl -p/etc/sysctl.d/99-kubernetes-cri.conf
</code></pre></div></div>

<p>The next step is to disable SELinux:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# setenforce 0

k8s-test.local# sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
</code></pre></div></div>

<p>And the installation of Kubernetes and CRI-O can proceed.</p>

<h3 id="installing-kubernetes-and-cri-o">Installing Kubernetes and CRI-O</h3>

<p>To install Kubernetes and CRI-O several ways can be used, in this guide there is the step-by-step guide where the user can do everything by himself or the alternative option, taking the easy road and running the ansible-playbook that will take care of almost everything.</p>

<h4 id="the-ansible-way">The ansible way</h4>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p>we are waiting for the <a href="https://github.com/cri-o/cri-o-ansible/pull/25">PR</a> to be merged in the official cri-o-ansible repository, meantime a fork in an alternative repository would be used. Also, note that the following commands are executed from a different place, in this case from a computer called <code class="highlighter-rouge">laptop</code>:</p>


</div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>laptop$ sudo yum install ansible -y

laptop# git clone https://github.com/ptrnull/cri-o-ansible

laptop# cd cri-o-ansible

laptop# git checkout fixes_k8s_1_16

laptop# ansible-playbook cri-o.yml -i 192.168.0.10,
</code></pre></div></div>

<p>Once the playbook ends the system would be ready for getting CRI-O configured.</p>

<h4 id="the-step-by-step-way">The step-by-step way</h4>

<p>If the ansible way was chosen, you may want to skip this section. Otherwise, let’s configure each piece.</p>

<p>The required packages may be installed in the system running the following command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# yum install btrfs-progs-devel container-selinux device-mapper-devel gcc git glib2-devel glibc-devel glibc-static gpgme-devel json-glib-devel libassuan-devel libgpg-error-devel libseccomp-devel make pkgconfig skopeo-containers tar wget -y
</code></pre></div></div>

<p>Install golang and the md2man packages:</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p>depending on the operating system running in your VM, it may be needed to change the name of the md2man golang package.</p>


</div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# yum install golang-github-cpuguy83-go-md2man golang -y
</code></pre></div></div>

<p>The following directories have to be created:</p>

<ul>
  <li>/usr/local/go</li>
  <li>/etc/systemd/system/kubelet.service.d/</li>
  <li>/var/lib/etcd</li>
  <li>/etc/cni/net.d</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# for d in "/usr/local/go /etc/systemd/system/kubelet.service.d/ /var/lib/etcd /etc/cni/net.d /etc/containers"; do mkdir -p $d; done
</code></pre></div></div>

<p>Clone the runc repository:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# git clone https://github.com/opencontainers/runc /root/src/github.com/opencontainers/runc
</code></pre></div></div>

<p>Clone the CRI-O repository:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# git clone https://github.com/cri-o/cri-o /root/src/github.com/cri-o/cri-o
</code></pre></div></div>

<p>Clone the CNI repository:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# git clone https://github.com/containernetworking/plugins /root/src/github.com/containernetworking/plugins
</code></pre></div></div>

<p>To build each part, a series of commands have to be executed, first building runc:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# cd /root/src/github.com/opencontainers/runc

k8s-test.local# export GOPATH=/root

k8s-test.local# make BUILDTAGS="seccomp selinux"

k8s-test.local# make install
</code></pre></div></div>

<p>And also runc has to be linked in the correct path:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# ln -sf /usr/local/sbin/runc /usr/bin/runc
</code></pre></div></div>

<p>Now building CRI-O (special focus on switching the branch):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# export GOPATH=/root

k8s-test.local# export GOBIN=/usr/local/go/bin

k8s-test.local# export PATH=/usr/local/go/bin:$PATH

k8s-test.local# cd /root/src/github.com/cri-o/cri-o

k8s-test.local# git checkout release-1.16

k8s-test.local# make

k8s-test.local# make install

k8s-test.local# make install.systemd

k8s-test.local# make install.config
</code></pre></div></div>

<p>CRI-O also needs the conmon software as a dependency:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# git clone http://github.com/containers/conmon /root/src/github.com/conmon

k8s-test.local# cd /root/src/github.com/conmon

k8s-test.local# make

k8s-test.local# make install
</code></pre></div></div>

<p>Now, the ContainerNetworking plugins have to be built and installed:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# cd /root/src/github.com/containernetworking/plugins

k8s-test.local# ./build_linux.sh

k8s-test.local# mkdir -p /opt/cni/bin

k8s-test.local# cp bin/* /opt/cni/bin/
</code></pre></div></div>

<p>The cgroup manager has to be changed in the CRI-O configuration from the value of <code class="highlighter-rouge">systemd</code> to <code class="highlighter-rouge">cgroupfs</code>, to get it done, the file <code class="highlighter-rouge">/etc/crio/crio.conf</code> has to be edited and the variable <code class="highlighter-rouge">cgroup_manager</code> has to be replaced from its original value of <code class="highlighter-rouge">systemd</code> to <code class="highlighter-rouge">cgroupfs</code> (it could be already set it up to that value, in that case this step can be skipped):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# vim /etc/crio/crio.conf
# group_manager = "systemd"
group_manager = "cgroupfs"
</code></pre></div></div>

<p>In the same file, the storage_driver is not configured, the variable <code class="highlighter-rouge">storage_driver</code> has to be uncommented and the value has to be changed from <code class="highlighter-rouge">overlay</code> to <code class="highlighter-rouge">overlay2</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# vim /etc/crio/crio.conf
#storage_driver = "overlay"
storage_driver = "overlay2"
</code></pre></div></div>

<p>Also related with the storage, the <code class="highlighter-rouge">storage_option</code> has to be configured to have the following value:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# vim /etc/crio/crio.conf
storage_option = [ "overlay2.override_kernel_check=1" ]
</code></pre></div></div>

<h3 id="preparing-cri-o">Preparing CRI-O</h3>

<p>CRI-O is the lightweight container runtime for Kubernetes. As it is pointed in the <a href="https://cri-o.io">CRI-O Website</a>:</p>

<blockquote>
  <p>CRI-O is an implementation of the Kubernetes CRI (Container Runtime Interface) to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative to using Docker as the runtime for Kubernetes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.</p>
</blockquote>

<blockquote>
  <p>CRI-O supports OCI container images and can pull from any container registry. It is a lightweight alternative to using Docker, Moby or rkt as the runtime for Kubernetes.</p>
</blockquote>

<p>The first step is to change the configuration of the <code class="highlighter-rouge">network_dir</code> parameter in the CRI-O configuration file, for doing so, the <code class="highlighter-rouge">network_dir</code> parameter in the <code class="highlighter-rouge">/etc/crio/crio.conf</code> file has to be changed to point to <code class="highlighter-rouge">/etc/crio/net.d</code></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local$ vim /etc/crio/crio.conf
[crio.network]
# Path to the directory where CNI configuration files are located.
network_dir = "/etc/crio/net.d/"
</code></pre></div></div>

<p>Also that directory has to be created:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local$ mkdir /etc/crio/net.d
</code></pre></div></div>

<p>The reason behind that change is because CRI-O and <code class="highlighter-rouge">kubeadm reset</code> don’t play well together, as <code class="highlighter-rouge">kubeadm reset</code> empties /etc/cni/net.d/. Therefore, it is good to change the <code class="highlighter-rouge">crio.network.network_dir</code> in <code class="highlighter-rouge">crio.conf</code> to somewhere kubeadm won’t touch. To get more information the following link [Running CRI-O with kubeadm] in the References section can be checked.</p>

<p>Now Kubernetes has to be configured to be able to talk to CRI-O, to proceed, a new file has to be created in <code class="highlighter-rouge">/etc/default/kubelet</code> with the following content:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KUBELET_EXTRA_ARGS=--feature-gates="AllAlpha=false,RunAsGroup=true" --container-runtime=remote --cgroup-driver=cgroupfs --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --runtime-request-timeout=5m
</code></pre></div></div>

<p>Now the systemd has to be reloaded:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# systemctl daemon-reload
</code></pre></div></div>

<p>CRI-O will use flannel network as it is recommended for multus so the following file has to be downloaded and configured:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# cd /etc/crio/net.d/

k8s-test.local# wget https://raw.githubusercontent.com/cri-o/cri-o/master/contrib/cni/10-crio-bridge.conf

k8s-test.local# sed -i 's/10.88.0.0/10.244.0.0/g' 10-crio-bridge.conf

</code></pre></div></div>

<p>As the previous code block has shown, the network used is <code class="highlighter-rouge">10.244.0.0</code>, now the crio service can be started and enabled:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k8s-test.local# systemctl enable crio
k8s-test.local# systemctl start crio
k8s-test.local# systemctl status crio
● crio.service - Container Runtime Interface for OCI (CRI-O)
   Loaded: loaded (/usr/local/lib/systemd/system/crio.service; enabled; vendor preset: disabled)
   Active: active (running) since mié 2019-10-02 16:17:06 CEST; 3s ago
     Docs: https://github.com/cri-o/cri-o
 Main PID: 15427 (crio)
   CGroup: /system.slice/crio.service
           └─15427 /usr/local/bin/crio

oct 02 16:17:06 k8s-test systemd[1]: Starting Container Runtime Interface for OCI (CRI-O)...
oct 02 16:17:06 k8s-test systemd[1]: Started Container Runtime Interface for OCI (CRI-O).
</code></pre></div></div>

<p>In the next posts, the <a href="/2019/KubeVirt_k8s_crio_from_scratch_installing_kubernetes.html">Kubernetes cluster will be set up</a>, together with the pod Network and also the KubeVirt with the virtual Machines deployments.</p>
:ET